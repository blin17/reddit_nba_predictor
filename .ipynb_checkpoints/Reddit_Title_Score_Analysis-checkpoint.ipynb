{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Analysis of /r/nba Posts\n",
    "@author: Brian Lin\n",
    "'''\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_csv_files(path):\n",
    "    pattern = r'^.*\\.csv$'\n",
    "    return [f for f in os.listdir(path) if bool(re.match(pattern,f))]\n",
    "    \n",
    "path = 'data/backlog/'\n",
    "csv_files = return_csv_files(path)\n",
    "df = pd.DataFrame()\n",
    "for csv in csv_files:\n",
    "    csv_df = pd.read_csv(path + csv)\n",
    "    df = pd.concat([df,csv_df])\n",
    "df['created_timestamp'] = pd.to_datetime(df.created, unit = 's')\n",
    "df.index = range(0,len(df))\n",
    "# 18,001 total examples with no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Randomly shuffle the dataframe and generates a training, cross-validation, and test set\n",
    "'''\n",
    "random_df = df.reindex(np.random.permutation(df.index))\n",
    "training_df = random_df.iloc[0:12000]\n",
    "cross_validation_df = random_df.iloc[12000:15000]\n",
    "test_df = random_df.iloc[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def front_page(score,n=150):\n",
    "    if score >= n:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "training_Y = training_df.score.apply(front_page)\n",
    "cross_validation_Y = cross_validation_df.score.apply(front_page)\n",
    "test_Y = test_df.score.apply(front_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words = pd.read_csv('common_words.txt', header = None)[0].values\n",
    "\n",
    "def remove_common_words(title, common_words):\n",
    "    tw = map(lambda d: d.strip(\".,:()[]!?\\\"\").lower(),title.split())\n",
    "    return [w for w in tw if w not in common_words]\n",
    "\n",
    "def generate_words_and_scores(titles, score, common_words):\n",
    "    title_words = []\n",
    "    score_words = []\n",
    "    for index in range(len(titles)):\n",
    "        title_word_list = remove_common_words(titles.iloc[index], common_words)\n",
    "        title_words.extend(title_word_list)\n",
    "        score_words.extend([scores.iloc[index]] * len(title_word_list))\n",
    "    title_words_s = pd.Series(title_words)\n",
    "    words_scores_df = pd.concat([title_words_s, pd.Series(score_words)], axis = 1)\n",
    "    words_scores_df.columns = ['word', 'score']\n",
    "    return words_scores_df\n",
    "\n",
    "titles = training_df.title\n",
    "scores = training_df.score\n",
    "word_df = generate_words_and_scores(titles,scores, common_words)\n",
    "word_df_agg = word_df.groupby('word').agg({'score':['mean','std','median','count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_sorted_median = word_df_agg[word_df_agg[('score','count')] > 10].sort_values(by=[('score','median')], ascending=False)\n",
    "top200 = np.array(words_sorted_median[:200].index)\n",
    "bottom200 = np.array(words_sorted_median[-200:].index)\n",
    "top_bottom200 = np.array(pd.concat([words_sorted_median[:200],words_sorted_median[-200:]]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'', u'&', u'+', u'--', u'/r/nba', u'10', u'100', u'11', u'12', u'13',\n",
       "       ...\n",
       "       u'wrong', u'x-post', u'yao', u'years', u'yet', u'york', u'you've',\n",
       "       u'young', u'zach', u'|'],\n",
       "      dtype='object', name=u'word', length=912)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df_agg[word_df_agg[('score','count')] > 20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-0c2172599380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtitle_features_bayes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_df_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_features_bayes_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-0c2172599380>\u001b[0m in \u001b[0;36mgenerate_feature_vector\u001b[0;34m(title_s, top_words, common_words)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtitle_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_common_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_feature_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-0c2172599380>\u001b[0m in \u001b[0;36mcreate_feature_row\u001b[0;34m(title, word_vec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Generates the feature word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feature_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeature_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Generates the feature word vectors\n",
    "def create_feature_row(title, word_vec):\n",
    "    feature_row = np.zeros((len(word_vec),))\n",
    "    for word in title:\n",
    "        if word in word_vec:\n",
    "            word_index = np.where(word_vec==word)[0][0]\n",
    "            feature_row[word_index] = 1\n",
    "    return feature_row\n",
    "\n",
    "def generate_feature_vector(title_s, top_words, common_words):\n",
    "    l = []\n",
    "    for title in title_s:\n",
    "        title_r = remove_common_words(title, common_words)\n",
    "        l.append(create_feature_row(title_r, top_words))\n",
    "    return l\n",
    "\n",
    "def sanitize_titles(title_s):\n",
    "    l = []\n",
    "    for title in title_s:\n",
    "        title_r = remove_common_words(title, common_words)\n",
    "        l.append(title_r)\n",
    "    return l\n",
    "    \n",
    "title_features_bayes_df = pd.DataFrame(generate_feature_vector(titles,word_df_agg.index,common_words), index = training_df.index)\n",
    "sum(title_features_bayes_df.sum(axis =1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_score(predicted_p, Y,threshold=0.5):\n",
    "    def greater_than_one_half(x):\n",
    "        if x > threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    predicted_Y = np.array([greater_than_one_half(x) for x in predicted_p])\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(predicted_Y)):\n",
    "        if predicted_Y[i] == 1 and Y[i] == 1:\n",
    "            true_pos +=1\n",
    "        elif predicted_Y[i] == 1 and Y[i] == 0:\n",
    "            false_pos +=1\n",
    "        elif predicted_Y[i] == 0 and Y[i] == 1:\n",
    "            true_neg +=1\n",
    "        elif predicted_Y[i] == 0 and Y[i] == 0:\n",
    "            false_neg +=1\n",
    "    \n",
    "    wrong = float(np.sum(np.abs(predicted_Y - Y)))\n",
    "    total = float(len(predicted_Y))\n",
    "    ans = dict()\n",
    "    ans['accuracy'] =  1-(wrong/total)\n",
    "    ans['true_pos'] = true_pos\n",
    "    ans['true_neg'] = true_neg\n",
    "    ans['false_pos'] = false_pos\n",
    "    ans['false_neg'] = false_neg\n",
    "    if true_pos == 0 and false_pos == 0:\n",
    "        ans['recall'] = float(true_pos) / float((true_pos + true_neg))\n",
    "    elif true_pos == 0 and true_neg == 0:\n",
    "        ans['precision'] = float(true_pos) / float((true_pos + false_pos))\n",
    "    else:\n",
    "        ans['precision'] = float(true_pos) / float((true_pos + false_pos))\n",
    "        ans['recall'] = float(true_pos) / float((true_pos + true_neg))\n",
    "        ans['f1_score'] = 2 * ans['precision'] * ans['recall'] / (ans['precision'] + ans['recall'] )\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_bayes_prob(words, X, Y):\n",
    "    pxy1 = {}\n",
    "    pxy0 = {}\n",
    "    for index in range(len(words)):\n",
    "        word = words[index]\n",
    "        pxy1[word] = float(sum((X.iloc[:,index] == 1) & (Y == 1)) +1) / float(sum(Y.values == 1) +2)\n",
    "        pxy0[word] = float(sum((X.iloc[:,index] == 1) & (Y == 0)) +1) / float(sum(Y.values == 0) +2)\n",
    "    return pxy1, pxy0\n",
    "\n",
    "def calculate_prob(sanitized_title_list,debug=False):\n",
    "    global bayes_pxy1,bayes_pxy0, py1, py0,pno1, pno0\n",
    "    pyx1 = 1.0\n",
    "    pyx0 = 1.0\n",
    "    for word in sanitized_title_list:\n",
    "        if word in bayes_pxy1:\n",
    "            pyx1 *= bayes_pxy1[word]\n",
    "            pyx0 *= bayes_pxy0[word]\n",
    "            if debug:\n",
    "                print 'yes'\n",
    "        else:\n",
    "            pyx1 *= pno1\n",
    "            pyx0 *= pno0\n",
    "            if debug:\n",
    "                print 'no'    \n",
    "        if debug:\n",
    "            print word, pyx1, pyx0\n",
    "    return pyx1*py1, pyx0*py0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_pxy1,bayes_pxy0 = generate_bayes_prob(word_df_agg.index, title_features_bayes_df, training_Y)\n",
    "Y = training_Y\n",
    "py1 = float(sum(Y.values == 1)) / len(Y)\n",
    "py0 = float(sum(Y.values == 0)) / len(Y)\n",
    "pno1 = 1/float(sum(Y.values == 1))\n",
    "pno0 = 1/float(sum(Y.values == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sanitized_titles = sanitize_titles(training_df.title)\n",
    "class_probs = []\n",
    "for i in range(len(sanitized_titles)):\n",
    "    title = sanitized_titles[i]\n",
    "    class_probs.append(calculate_prob(title))\n",
    "probs = pd.DataFrame(class_probs)\n",
    "sum(probs[0] > probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs.index = training_df.index\n",
    "pd.concat([training_df[probs[0] > probs[1]][['title','score']], probs], axis=1, join= 'inner').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "#2016-06-08 06:26:36\n",
    "training_df[(Y.values == 0)].created.apply(lambda d: d.hour).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df[(predicted_Y3 == 1) & (Y.values == 1)][['title','score','domain','created']].domain.value_counts()[:15].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_predicted3 = probs.apply(lambda d: 1 if d[0] > d[1] else 0, axis = 1)\n",
    "predicted_Y3 = np.array(Y_predicted3)\n",
    "p_score = prediction_score(predicted_Y3, Y.values)\n",
    "p_score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
