{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Analysis of /r/nba Posts\n",
    "@author: Brian Lin\n",
    "'''\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import Bayes_Helper as bayes\n",
    "import Multinomial_Helper as multih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_csv_files(path):\n",
    "    pattern = r'^.*\\.csv$'\n",
    "    return [f for f in os.listdir(path) if bool(re.match(pattern,f))]\n",
    "    \n",
    "path = 'data/backlog/'\n",
    "csv_files = return_csv_files(path)\n",
    "df = pd.DataFrame()\n",
    "for csv in csv_files:\n",
    "    csv_df = pd.read_csv(path + csv)\n",
    "    df = pd.concat([df,csv_df])\n",
    "df['created'] = pd.to_datetime(df.created, unit = 's')\n",
    "df.index = range(0,len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Randomly shuffle the dataframe and generates a training, cross-validation, and test set\n",
    "'''\n",
    "random_df = df.reindex(np.random.permutation(df.index))\n",
    "training_df = random_df.iloc[0:12000]\n",
    "cross_validation_df = random_df.iloc[12000:15000]\n",
    "test_df = random_df.iloc[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoffs = [50, 150, 350]\n",
    "Y_training = training_df.score.apply(lambda d: multih.bucketize(cutoffs,d))\n",
    "Y_cross_validation = cross_validation_df.score.apply(lambda d: multih.bucketize(cutoffs,d))\n",
    "Y_test = test_df.score.apply(lambda d: multih.bucketize(cutoffs,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = (bayes.generate_words_and_scores(training_df.title, training_df.score)\n",
    "                 .groupby('word').agg({'score':['mean','std','median','count']}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words[('score','bucket')] = words[('score','median')].apply(lambda d: multih.bucketize(cutoffs,d))\n",
    "bucketize_words = words[words[('score','count')] > 20].sort_values(by=[('score','bucket'),('score','count')], ascending=False)\n",
    "top200ineachbucket = []\n",
    "for i in range(len(cutoffs) + 1):\n",
    "    if i == 2:\n",
    "        bucket = bucketize_words[bucketize_words[('score','bucket')] == i][:400]\n",
    "    else:\n",
    "        bucket = bucketize_words[bucketize_words[('score','bucket')] == i][:200]\n",
    "    top200ineachbucket.append(bucket)\n",
    "top200ineachbucket = pd.concat(top200ineachbucket).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 250\n",
    "top250 = words[words['score','count'] > 20].sort_values(by=('score','median'),ascending=False)[:n]\n",
    "bot250 = words[words['score','count'] > 20].sort_values(by=('score','median'),ascending=True)[:n]\n",
    "topbot500 = pd.concat([top250,bot250])\n",
    "topbot500_words = topbot500.index.values\n",
    "all_words = words.index.values\n",
    "word_vector_trained_upon= topbot500_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f500words = words[words['score','count'] > 20].sort_values(by=('score','count'),ascending=False)[:500].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(titles):\n",
    "    words = (bayes.generate_words_and_scores(training_df.title, training_df.score)\n",
    "                 .groupby('word').agg({'score':['mean','std','median','count']}))\n",
    "    words[('score','bucket')] = words[('score','median')].apply(lambda d: multih.bucketize(cutoffs,d))\n",
    "    bucketize_words = words[words[('score','count')] > 20].sort_values(by=[('score','bucket'),('score','count')], ascending=False)\n",
    "    top200ineachbucket = []\n",
    "    for i in range(len(cutoffs) + 1):\n",
    "        if i == 2:\n",
    "            bucket = bucketize_words[bucketize_words[('score','bucket')] == i][:400]\n",
    "        else:\n",
    "            bucket = bucketize_words[bucketize_words[('score','bucket')] == i][:200]\n",
    "        top200ineachbucket.append(bucket)\n",
    "    top200ineachbucket = pd.concat(top200ineachbucket).index.values\n",
    "    return top200ineachbucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bayes_weights = [.9,.8,1.2,.6]\n",
    "#bayes_weights = [1,1,1,1]\n",
    "def pxy_table(X_data, Y_data, words, num_i):\n",
    "    '''\n",
    "    p(word|y=i) = # of times y = i where word appears / # of time y = i\n",
    "    '''\n",
    "    pxyi = [{} for i in range(num_i)]\n",
    "    for index in range(len(words)):\n",
    "        for i in range(len(pxyi)):\n",
    "            word = words[index]\n",
    "            table = pxyi[i]\n",
    "            num = float(((X_data.iloc[:,index] == 1) & (Y_data == i)).sum() +1) \n",
    "            den = float((Y_data == i).sum() +2)\n",
    "            table[word] = num/den\n",
    "    return pxyi\n",
    "\n",
    "def py_table(Y_data, num_i):\n",
    "    py = []\n",
    "    for i in range(num_i):\n",
    "        py.append(float((Y_data == i).sum())/ len(Y_data))\n",
    "    return py\n",
    "\n",
    "def bayes_prob(x_hour, x_titles, pxy_table, py_table, pno):\n",
    "    titles_prob = []\n",
    "    for title_i in range(len(x_titles)):\n",
    "        title = x_titles[title_i]\n",
    "        title_prob = [1.0 for _ in range(len(py_table))]\n",
    "        for word in title:\n",
    "            for i in range((len(title_prob))):\n",
    "                if word in pxy_table[i]:\n",
    "                    title_prob[i] *= pxy_table[i][word]\n",
    "                #else:\n",
    "                #    title_prob[i] *= pno[i]\n",
    "        hour = X_hour.iloc[title_i]\n",
    "        title_prob = [(phour[i+hour*4]) * (bayes_weights[i]) * (title_prob[i]) for i in range(len(py_table))]\n",
    "        titles_prob.append(title_prob)\n",
    "    return titles_prob\n",
    "\n",
    "def pno_table(Y_data, num_i):\n",
    "    pno = []\n",
    "    for i in range(num_i):\n",
    "        pno.append(1 / float(sum(Y_data.values == i) +2))\n",
    "    return pno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041583333333333333"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: This changes the word vector we train upon\n",
    "word_vector_trained_upon= top200ineachbucket\n",
    "\n",
    "\n",
    "X_training = bayes.generate_feature_vector((training_df.title), word_vector_trained_upon, bayes.common_words)\n",
    "X_training_df = pd.DataFrame(X_training)\n",
    "\n",
    "# number of training examples without any features mapped\n",
    "sum(X_training_df.sum(axis=1) == 0)/float(len(X_training_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pxy = pxy_table(X_training_df, Y_training.values, word_vector_trained_upon, len(cutoffs)+1)\n",
    "py = py_table(Y_training, len(cutoffs)+1)\n",
    "pno = pno_table(Y_training, len(cutoffs)+1)\n",
    "\n",
    "\n",
    "X_hour = training_df.created.apply(lambda d: d.hour).rename('hour')\n",
    "phour = ((pd.concat([hour,Y_training], axis=1).groupby(['hour','score']).size()\n",
    "          / pd.concat([hour,Y_training], axis=1).groupby(['hour']).size())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_p_values(X_data, Y_data, bag_of_words, cutoffs):\n",
    "    \n",
    "    X_training = bayes.generate_feature_vector((X_data.title), bag_of_words, bayes.common_words)\n",
    "    X_training_df = pd.DataFrame(X_training)\n",
    "    \n",
    "    pxy = pxy_table(X_training_df, Y_data.values, bag_of_words, len(cutoffs)+1)\n",
    "    py = py_table(Y_data, len(cutoffs)+1)\n",
    "    pno = pno_table(Y_data, len(cutoffs)+1)\n",
    "    \n",
    "    X_hour = X_data.created.apply(lambda d: d.hour).rename('hour')\n",
    "    phour = ((pd.concat([X_hour,Y_data], axis=1).groupby(['hour','score']).size()\n",
    "              / pd.concat([X_hour,Y_data], axis=1).groupby(['hour']).size())).values\n",
    "    return pxy, py, pno, py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_by_domain(X_data,Y_data):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predicted_Y, Y):\n",
    "    return sum(predicted_Y == Y)/float(len(predicted_Y))\n",
    "def accuracy_on_set(X_data, Y_data):\n",
    "    X_hour = training_df.created.apply(lambda d: d.hour).rename('hour')\n",
    "    probs_Y = pd.DataFrame(bayes_prob(X_hour, bayes.sanitize_and_split_titles(X_data.title),pxy,py,pno))\n",
    "    probs_Y.index = Y_data.index\n",
    "    prediction_Y = probs_Y.apply(lambda d: d.argmax(),axis=1)\n",
    "    prediction_Y.index = Y_data.index\n",
    "    return accuracy(prediction_Y, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.57875\n",
      "validation accuracy: 0.526333333333\n",
      "test accuracy: 0.551149616794\n"
     ]
    }
   ],
   "source": [
    "#Goal is 65% Accuracy\n",
    "\n",
    "print 'training accuracy:', accuracy_on_set(training_df, Y_training)\n",
    "print 'validation accuracy:', accuracy_on_set(cross_validation_df, Y_cross_validation)\n",
    "print 'test accuracy:', accuracy_on_set(test_df, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs_Y = pd.DataFrame(bayes_prob(training_df, bayes.sanitize_and_split_titles(training_df.title),pxy,py,pno))\n",
    "probs_Y.index = Y_training.index\n",
    "prediction_Y = probs_Y.apply(lambda d: d.argmax(),axis=1)\n",
    "prediction_Y.index = Y_training.index\n",
    "\n",
    "debug_df = pd.concat([training_df.title,prediction_Y, Y_training], axis=1)\n",
    "debug_df.title = bayes.sanitize_and_split_titles(training_df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7252\n",
       "3    2455\n",
       "2    1200\n",
       "1    1093\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.255654\n",
       "3    0.633809\n",
       "2    0.755833\n",
       "1    0.675206\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_Y[Y_training != prediction_Y].value_counts()/prediction_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.257803\n",
       "1    0.796678\n",
       "2    0.756037\n",
       "3    0.494944\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training[Y_training != prediction_Y].value_counts()/Y_training.value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
